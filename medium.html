<!DOCTYPE html>
<html>
<head>
<meta charset='utf-8'>
<title>https://medium.com/analytics-vidhya/understanding-the-vision-transformer-and-counting-its-parameters-988a4ea2b8f3</title>
</head>
<body>
<pre>Understanding the Vision Transformer and Counting Its Parameters | by Andrei-Cristian Rad | Analytics Vidhya | MediumPublished inAnalytics VidhyaAndrei-Cristian RadFollowDec 2, 2020·7 min readUnderstanding the Vision Transformer and Counting Its ParametersPhoto by Simon Migaj on UnsplashIn this post, I will share my understanding of the Vision Transformer architecture. All the drawings in this post are original content, based on the knowledge from the paper and other tutorials which will be referred to where appropriate.Transformer architectures have been a major breakthrough in Natural Language Processing (NLP) tasks since being proposed in 2016. Google’s BERT and Open AI’s GPT-2 / GPT-3 architectures have been the state-of-the-art solutions for various tasks, including language modeling, text summarization, and question answering.The aim was to prove that the recurrent neural networks can be completely replaced, and solutions can be developed using only attention mechanisms — hence the pun in the Transformer paper title. Detailing the way the original Transformer architecture operates is beyond the scope of this article, but there are plenty of solid tutorials and videos that explain the way it works.The focus of this article is presenting an overview of the Vision Transformer (ViT) architecture, proposed in a new paper for Google, submitted for review for ICLR 2021. Although there were previous attempts at using attention mechanisms for computer vision tasks (this, this, or this), ViT is the most promising architecture, both from the scalability and the efficiency point of view.Vision Trnasformer ArchitecutreThe architecture contains 3 main components.Patch embedding.Feature extraction via stacked transformer encoders.The classification head.Each component will be detailed in the next paragraphs, with the focus on the transformations and the trainable parameters. The first dimension of some tensors is b, which represents the batch size, but this and the broadcasting operations will be ignored in the comments for simplicity.Patch EmbeddingPatch EmbeddingIn the first step, an input image of shape (height, width, channels) is embedded into a feature vector of shape (n+1, d), following a sequence of transformations. This corresponds to equation (1) from the paper:The image is split into n square patches of shape (p, p, c), where p is a pre-defined parameter, in raster order (left to right, up to down).The patches are flattened, resulting in n line vectors of shape (1, p²*c).The flattened patches are multiplied with a trainable embedding tensor of shape (p²*c, d), which learns to linearly project each flat patch to dimension d. This dimension d is constant in the architecture and is used in most of the components. The result is n embedded patches of shape (1, d).A learnable [cls] token of shape (1, d) is prepended to the sequence of patch embeddings. The idea of this token is from the BERT paper, where only the last representation corresponding to this token (output of transformer L) is fed through the classification layers. Intuitively, this represents an aggregate of the representations of the patches.A trainable positional embedding tensor, Eₚₒₛ, with the same shape, (n+1, d), is added to the concatenated sequence of projections. This tensor learns 1D positional information for each of the patches, in order to add a spatial representation of each patch within the sequence.The result, z₀, is the first input to the stacked transformer encoders. The L stacked encoders represent the second component of the architecture. Each transformer takes as input features represented as an (n+1, d) tensor, and produces an output of the same dimension.Transformer EncoderTransformer encoderIn the second step, the network learns more abstract features from the embedded patches, using a stack of L transformer encoders. This corresponds to equations (2) and (3) from the paper.The encoder component contains a multi-headed attention (MHA) mechanism and a 2-layer MLP, with layer normalization and residual connections in between.Layer normalization helps to stabilize hidden state dynamics and to reduce the training time. It is done by scaling with the mean and standard deviation for each training example (as opposed to the batch norm where this is done per feature). The resulting features are multiplied with a scaling factor and added to a shifting factor, both learnable during training.Residual connections offer gradients alternative paths, in order to solve the problem of vanishing gradients in very deep architectures.The trainable weights in this component lie inside the MHA mechanism and the MLP weights. Since the MLP has 2 layers (hidden and output), there will be two weight matrices:Wₕ of shape (d, dₘₗₚ)Wₒ of shape (dₘₗₚ, d)The attention mechanismThe multi-head attention (MHA) step, included in each of the L stacked transformers, corresponds to equations (5), (6), (7), and (8) from the paper’s appendix.The hidden state from the previous encoder is split into K heads, resulting in K feature tensors of shape (n, dₕ). From the multi-head attention intuition, multiple heads allow the mechanism to learn from different aspects of the abstract representation.Each is multiplied with 3 trainable matrices Qi, Ki, Vi, of shape (dₕ, dₕ). This is equivalent to equation 5, as in U=(d, 3dₕ) there are exactly 3 matrices for each head, each of shape (dₕ, dₕ).Qi, Ki, and Vi represent the projection of the input in 3 sub-spaces. We can think of each line in Q as a learned projection of the patch we are interested in, and lines in K as other patches we compare Q to. V and K are learned to express importance, or weights, for features in V to compute the final “attention”.Following, on each head, the scaled dot-product attention tensor (A) is computed as a softmax of the multiplication between the Ki and Qi matrices, normalized with the square root of the dimension of the head. The i-th row in this matrix is a probability distribution function of the attention for query i, meaning to keys of which other patches is the query of patch i most similar to.The self-attention is the product between A and v, which has the shape (n+1, dₕ). The element at row i and column j is the weighted average of feature j by the pdf on line i in A.The self-attention matrices are concatenated on the second dimension, resulting in an (n+1, d) tensor, which is then run through a single linear layer, by effectively multiplying it with a (d, d) trainable tensor. This linear layer is very important, as it allows features to be learned as aggregates from all the heads.Classification HeadClassification HeadAs stated previously, only the last representation of the [cls] token is used in the classification head. For pre-training, a 2-layer MLP is used, therefore there are two weight matrices — Wₕ of shape (d, dₘₗₚ) and Wₒ of shape (dₘₗₚ, d). For fine-tuning, a single linear layer is used, therefore there is only a single tensor, of shape (d, n_cls). In each case, the final output of the network is a vector of shape (1, n_cls), containing the probabilities associated with each of the n_cls classes.Trainable ParametersVision Transformer Paramteres [1]Let us take the ViT-Base architecture and calculate the number of parameters. trainable tensor in the architecture.In the patch embedding step, the two embedding matrices account for 786.432 parameters.p²*c*d + (n+1)*d = 256*3*768 + 256*768 = 786.432In the encoder stack, there are a total of 84.943.656 parameters.L*(k*d*3*dₕ + d*d + d*dₘₗₚ + dₘₗₚ*d) = 12*(12*768*3*64 + 768*768 + 2*768*3072) = 12*(1.769.472 + 589.824 + 4.718.592) = 12*589.824(3+1+8)= 12*589.824*12= 84.934.656For fine-tuning (on ImageNet, for instance), there are other 768.000 parameters.d*ncls = 768 * 1000 = 768.000Adding those, we obtain around 86M, as listed in the paper.ResultsThe ViT architecture has obtained state-of-the-art performance on ImageNet, when pre-trained on JFT-300M. However, the authors noted that the performance decreases drastically when using small datasets for pre-training, and that larger models are better suited for larger datasets. This issue most likely arises because of the lack of inductive bias in transformer networks. They are able to process any sequence, without knowing its order relationship.Accuracy vs. Pre-training Dataset [1]ConclusionsFor me, being able to visualize the transformations and the flow in this architecture has been of great help. Hopefully, it is the same in your case. Cheers!References[1] A. Dostovitskiy et. al., An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020) ICLR 2021 (under review)[2] A. Vaswani et. al., Attention is All You Need (2017), Proceedings of NIPS2017[3] J. Devlin et. al., BERT: Pre-training of deep bidirectional transformers for language understanding (2018)Deep LearningComputer VisionTransformersDeep Neural Networks----1More from Analytics VidhyaAnalytics Vidhya is a community of Analytics and Data Science professionals. We are building the next-gen data science ecosystem https://www.analyticsvidhya.comRead more from Analytics VidhyaAboutHelpTermsPrivacyGet the Medium appGet unlimited accessAndrei-Cristian Rad15 FollowersPassionate about my craft and highly motivated to bring the AI revolution to everyone's ears.FollowHelpStatusWritersBlogCareersPrivacyTermsAboutText to speech









































</pre>
</body>
</html>