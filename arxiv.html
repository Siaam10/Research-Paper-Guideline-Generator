<!DOCTYPE html>
<html>
<head>
<meta charset='utf-8'>
<title>https://arxiv.org/abs/2202.06709</title>
</head>
<body>
<pre>



[2202.06709] How Do Vision Transformers Work?















































Skip to main content






We gratefully acknowledge support fromthe Simons Foundation and member institutions.





 > cs > arXiv:2202.06709
  





Help | Advanced Search




All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text




Search















open search






GO



open navigation menu


quick links

Login
Help Pages
About













Computer Science > Computer Vision and Pattern Recognition


arXiv:2202.06709 (cs)
    




  
  
  
    
  
  
    
    
  

  [Submitted on 14 Feb 2022 (v1), last revised 8 Jun 2022 (this version, v4)]
Title:How Do Vision Transformers Work?
Authors:Namuk Park, Songkuk Kim
Download a PDF of the paper titled How Do Vision Transformers Work?, by Namuk Park and 1 other authors
Download PDF

Abstract:  The success of multi-head self-attentions (MSAs) for computer vision is now
indisputable. However, little is known about how MSAs work. We present
fundamental explanations to help better understand the nature of MSAs. In
particular, we demonstrate the following properties of MSAs and Vision
Transformers (ViTs): (1) MSAs improve not only accuracy but also generalization
by flattening the loss landscapes. Such improvement is primarily attributable
to their data specificity, not long-range dependency. On the other hand, ViTs
suffer from non-convex losses. Large datasets and loss landscape smoothing
methods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors.
For example, MSAs are low-pass filters, but Convs are high-pass filters.
Therefore, MSAs and Convs are complementary; (3) Multi-stage neural networks
behave like a series connection of small individual models. In addition, MSAs
at the end of a stage play a key role in prediction. Based on these insights,
we propose AlterNet, a model in which Conv blocks at the end of a stage are
replaced with MSA blocks. AlterNet outperforms CNNs not only in large data
regimes but also in small data regimes. The code is available at
this https URL.

    




Comments:
ICLR 2022 (Spotlight)


Subjects:

Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

Cite as:
arXiv:2202.06709 [cs.CV]


 
(or 
arXiv:2202.06709v4 [cs.CV] for this version)
          


 

https://doi.org/10.48550/arXiv.2202.06709



Focus to learn more




                arXiv-issued DOI via DataCite
              







Submission history From: Namuk Park [view email]
      
[v1]
  Mon, 14 Feb 2022 13:58:43 UTC (645 KB)
[v2]
  Sun, 27 Feb 2022 19:35:00 UTC (645 KB)
[v3]
  Wed, 11 May 2022 16:51:05 UTC (628 KB)[v4]
Wed, 8 Jun 2022 12:41:38 UTC (542 KB)





Full-text links:
Download:

Download a PDF of the paper titled How Do Vision Transformers Work?, by Namuk Park and 1 other authors
PDF
Other formats




    Current browse context: cs.CV


< prev

  |  

next >


new
 | 
recent
 | 
2202

    Change to browse by:
    
cs
cs.LG




References & Citations

NASA ADSGoogle Scholar
Semantic Scholar




a
export BibTeX citation
Loading...




BibTeX formatted citation
×


loading...


Data provided by: 





Bookmark

















Bibliographic Tools

Bibliographic and Citation Tools






Bibliographic Explorer Toggle



Bibliographic Explorer (What is the Explorer?)







Litmaps Toggle



Litmaps (What is Litmaps?)







scite.ai Toggle



scite Smart Citations (What are Smart Citations?)








Code, Data, Media

Code, Data and Media Associated with this Article






DagsHub Toggle



DagsHub (What is DagsHub?)







Links to Code Toggle



Papers with Code (What is Papers with Code?)







ScienceCast Toggle



ScienceCast (What is ScienceCast?)









Demos

Demos






Replicate Toggle



Replicate (What is Replicate?)







Spaces Toggle



Hugging Face Spaces (What is Spaces?)







Related Papers

Recommenders and Search Tools






Link to Influence Flower



Influence Flower (What are Influence Flowers?)







Connected Papers Toggle



Connected Papers (What is Connected Papers?)







Core recommender toggle



CORE Recommender (What is CORE?)





Author
Venue
Institution
Topic














        About arXivLabs
      



arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.










Which authors of this paper are endorsers? |
    Disable MathJax (What is MathJax?)
    












About
Help





contact arXivClick here to contact arXiv
 Contact


subscribe to arXiv mailingsClick here to subscribe
 Subscribe











Copyright
Privacy Policy




Web Accessibility Assistance


arXiv Operational Status 
                    Get status notifications via
                    email
                    or slack





 






</pre>
</body>
</html>