<!DOCTYPE html>
<html>
<head>
<meta charset='utf-8'>
<title>https://viso.ai/deep-learning/vision-transformer-vit/</title>
</head>
<body>
<pre> Vision Transformers (ViT) in Image Recognition: Full Guide - viso.ai              Skip to content        Blog  Computer Vision  Applications  Edge AI  Deep Learning  Company News     Blog Home      Explore Blog      Contribute      Follow us   View all PlatformTrainDevelopDeployOperate Data Collection CubeBuilding Blocks​ Instance classicDevice Enrollment Chart comboMonitoring Dashboards Area customVideo Annotation​ Flow modelerApplication Editor​ Object storageDevice Management Tool kitRemote Maintenance Carbon for IBM productModel Training DevelopmentApplication Library Load balancer globalDeployment Manager SecurityUnified Security Center AI Model Library SettingsConfiguration Manager Gateway securityIoT Edge Gateway RulePrivacy-preserving AIReady to get started? Get a demo Layers Overview Document Whitepaper User admin Expert ServicesWhy Viso SuitePricing  Search   Close   Get Demo        Request Demo   Deep LearningVision Transformers (ViT) in Image Recognition – 2023 Guide       Deliver AI vision faster        Gaudenz Boesch   AboutViso Suite is the all-in-one solution for teams to build, deliver, scale computer vision applications.  Contents Need Computer Vision?Viso Suite is only all-in-one business platform to build and deliver computer vision without coding. Learn more.Vision Transformer (ViT) have recently emerged as a competitive alternative to Convolutional Neural Networks (CNNs) that are currently state-of-the-art in different image recognition computer vision tasks. ViT models outperform the current state-of-the-art (CNN) by almost x4 in terms of computational efficiency and accuracy.Transformer models have become the de-facto status quo in Natural Language Processing (NLP). For example, the popular ChatGPT AI chatbot is a transformer-based language model. Specifically, it is based on the GPT (Generative Pre-trained Transformer) architecture, which uses self-attention mechanisms to model the dependencies between words in a text.In computer vision research, there has recently been a rise in interest in Vision Transformer (ViTs) and Multilayer Perceptrons (MLPs).This article will cover the following topics:What is a Vision Transformer (ViT)?Using ViT models in Image RecognitionHow do Vision Transformers work?Use Cases and applications of Vision Transformers About us: Viso.ai provides the leading end-to-end Computer Vision Platform Viso Suite. Our solution enables organizations worldwide to seamlessly build and deliver video image recognition applications. Get a demo for your company.Viso Suite provides end-to-end software for AI vision. Vision Transformer (ViT) in Image RecognitionWhile the Transformer architecture has become the highest standard for tasks involving Natural Language Processing (NLP), its use cases relating to Computer Vision (CV) remain only a few. In many computer vision tasks, attention is either used in conjunction with convolutional networks (CNN) or used to substitute certain aspects of convolutional networks while keeping their entire composition intact. Popular image recognition algorithms include ResNet, VGG, YOLOv3, and YOLOv7. The concept of widely popular Convolutional Neural Networks (CNN)However, this dependency on CNN is not mandatory, and a pure transformer applied directly to sequences of image patches can work exceptionally well on image classification tasks. Performance of Vision Transformers in Computer VisionVision Transformers (ViT) have recently achieved highly competitive performance in benchmarks for several computer vision applications, such as image classification, object detection, and semantic image segmentation.CSWin Transformer is an efficient and effective Transformer-based backbone for general-purpose vision tasks that uses a new technique called “Cross-Shaped Window self-attention” to analyze different parts of the image at the same time, which makes it much faster.The CSWin Transformer has surpassed previous state-of-the-art methods, such as the Swin Transformer. In benchmark tasks, CSWIN achieved excellent performance, including 85.4% Top-1 accuracy on ImageNet-1K, 53.9 box AP and 46.4 mask AP on the COCO detection task, and 52.2 mIOU on the ADE20K semantic segmentation task. What is a Vision Transformer (ViT)?The Vision Transformer (ViT) model architecture was introduced in a research paper published as a conference paper at ICLR 2021 titled “An Image is Worth 16*16 Words: Transformers for Image Recognition at Scale”. It was developed and published by Neil Houlsby, Alexey Dosovitskiy, and 10 more authors of the Google Research Brain Team.The fine-tuning code and pre-trained ViT models are available on the GitHub of the Google Research team. You find them here. The ViT models were pre-trained on the ImageNet and ImageNet-21k datasets. Origin and history of vision transformer modelsIn the following, we highlight some of the most significant vision transformers that have been developed over the years. They are based on the transformer architecture, which was originally proposed for natural language processing (NLP) in 2017.DateModelDescriptionVision Transformer?2017 JunTransformerA model based solely on an attention mechanism. It demonstrated excellent performance on NLP tasks.No2018 OctBERTPre-trained transformer models started dominating the NLP field.No2020 MayDETRDETR is a simple yet effective framework for high-level vision that views object detection as a direct set prediction problem.Yes2020 MayGPT-3The GPT-3 is a huge transformer model with 170B parameters that takes a significant step towards a general NLP model.No2020 JuliGPTThe transformer model, originally developed for NLP, can also be used for image pre-training.Yes2020 OctViTPure transformer architectures that are effective for visual recognition.Yes2020 DecIPT/SETR/CLIPTransformers have been applied to low-level vision, segmentation, and multimodality tasks, respectively.Yes2021 – todayViT VariantsThere are several ViT variants, including DeiT, PVT, TNT, Swin, and CSWin (2022).Yes Are Transformers a Deep Learning method?A transformer in machine learning is a deep learning model that uses the mechanisms of attention, differentially weighing the significance of each part of the input sequence of data. Transformers in machine learning are composed of multiple self-attention layers. They are primarily used in the AI subfields of natural language processing (NLP) and computer vision (CV).Transformers in machine learning hold strong promises toward a generic learning method that can be applied to various data modalities, including the recent breakthroughs in computer vision achieving state-of-the-art standard accuracy with better parameter efficiency. Vision Transformer and Image ClassificationImage classification is a fundamental task in computer vision that involves assigning a label to an image based on its content. Over the years, deep convolutional neural networks (CNNs) like YOLOv7 have been the state-of-the-art method for image classification.However, recent advancements in transformer architecture, which was originally introduced for natural language processing (NLP), have shown great promise in achieving competitive results in image classification tasks. A practical example of Image Classification in Medical Imaging and HealthcareAn example is CrossViT, a cross-attention Vision Transformer for Image Classification. Computer vision research indicates that when pre-trained with a sufficient amount of data, ViT models are at least as robust as ResNet models.Other papers showed that Vision Transformer Models have great potential for privacy-preserving image classification and outperform state-of-the-art methods in terms of robustness against attacks and classification accuracy. Difference between CNN and ViT (ViT vs. CNN)Vision Transformer (ViT) achieves remarkable results compared to convolutional neural networks (CNN) while obtaining substantially fewer computational resources for pre-training. In comparison to convolutional neural networks (CNN), Vision Transformer (ViT) show a generally weaker inductive bias resulting in increased reliance on model regularization or data augmentation (AugReg) when training on smaller datasets.The ViT is a visual model based on the architecture of a transformer originally designed for text-based tasks. The ViT model represents an input image as a series of image patches, like the series of word embeddings used when using transformers to text, and directly predicts class labels for the image. ViT exhibits an extraordinary performance when trained on enough data, breaking the performance of a similar state-of-art CNN with 4x fewer computational resources. CNN vs. ViT: FLOPs and throughput comparison of CNN and Vision Transformer Models  – SourceThese transformers have high success rates when it comes to NLP models and are now also applied to images for image recognition tasks. CNN use pixel arrays, whereas ViT splits the input images into visual tokens. The visual transformer divides an image into fixed-size patches, correctly embeds each of them, and includes positional embedding as an input to the transformer encoder. Moreover, ViT models outperform CNNs by almost four times when it comes to computational efficiency and accuracy.The self-attention layer in ViT makes it possible to embed information globally across the overall image. The model also learns on training data to encode the relative location of the image patches to reconstruct the structure of the image.The transformer encoder includes the following:Multi-Head Self Attention Layer (MSP): This layer concatenates all the attention outputs linearly to the right dimensions. The many attention heads help train local and global dependencies in an image.Multi-Layer Perceptrons (MLP) Layer: This layer contains a two-layer with Gaussian Error Linear Unit (GELU).Layer Norm (LN): This is added prior to each block as it does not include any new dependencies between the training images. This thereby helps improve the training time and overall performance.Moreover, residual connections are included after each block as they allow the components to flow through the network directly without passing through non-linear activations.In the case of image classification, the MLP layer implements the classification head. It does it with one hidden layer at pre-training time and a single linear layer for fine-tuning. What is self-attention of Vision Transformer?The self-attention mechanism is a key component of the transformer architecture, which is used to capture long-range dependencies and contextual information in the input data. The self-attention mechanism allows a ViT model to attend to different regions of the input data, based on their relevance to the task at hand.Therefore, the self-attention mechanism computes a weighted sum of the input data, where the weights are computed based on the similarity between the input features. This allows the model to give more importance to the relevant input features, which helps it to capture more informative representations of the input data.Hence, self-attention is a computational primitive used to quantify pairwise entity interactions that help a network to learn the hierarchies and alignments present inside input data. Attention has proven to be a key element for vision networks to achieve higher robustness. Raw images (left) with attention maps of the ViT-S/16 model (right). – SourceWhat are attention maps of ViT?The attention maps of Vision Transformer (ViT) are matrices that represent the importance of different parts of an input image to different parts of the model’s learned representations. In ViT, the entire image of the input data is first divided into non-overlapping patches, which are then flattened and fed into the transformer encoder (more about the architecture below).Attention maps refer to the visualizations of the attention weights that are calculated between each token (or patch) in the image and all other tokens. These attention maps are calculated using a self-attention mechanism, where each token attends to all other tokens to obtain a weighted sum of their representations.The attention maps can be visualized as a grid of heatmaps, where each heatmap represents the attention weights between a given token and all other tokens. The brighter the color of a pixel in the heatmap, the higher the attention weight between the corresponding tokens. By analyzing the attention maps, we can gain insights into which parts of the image are most important for the classification task at hand. Visualization of attention maps of ViT on images from ImageNet-A- Source Vision Transformer ViT ArchitectureSeveral vision transformer models have been proposed in the literature. The overall structure of the vision transformer architecture consists of the following steps:Split an image into patches (fixed sizes)Flatten the image patchesCreate lower-dimensional linear embeddings from these flattened image patchesInclude positional embeddingsFeed the sequence as an input to a state-of-the-art transformer encoderPre-train the ViT model with image labels, which is then fully supervised on a big datasetFine-tune the downstream dataset for image classification Vision Transformer ViT Architecture – SourceVision Transformers (ViT) is an architecture that uses self-attention mechanisms to process images. The Vision Transformer Architecture consists of a series of transformer blocks. Each transformer block consists of two sub-layers: a multi-head self-attention layer and a feed-forward layer.The self-attention layer calculates attention weights for each pixel in the image based on its relationship with all other pixels, while the feed-forward layer applies a non-linear transformation to the output of the self-attention layer. The multi-head attention extends this mechanism by allowing the model to attend to different parts of the input sequence simultaneously.ViT also includes an additional patch embedding layer, which divides the image into fixed-size patches and maps each patch to a high-dimensional vector representation. These patch embeddings are then fed into the transformer blocks for further processing.The final output of the ViT architecture is a class prediction, obtained by passing the output of the last transformer block through a classification head, which typically consists of a single fully connected layer. Performance benchmark comparison of Vision Transformers (ViT) with ResNet and MobileNet when trained from scratch on ImageNet. – SourceWhile the ViT full-transformer architecture is a promising option for vision processing tasks, the performance of ViTs is still inferior to that of similar-sized CNN alternatives (such as ResNet) when trained from scratch on a mid-sized dataset such as ImageNet. Overall, the ViT architecture allows for a more flexible and efficient way to process images, without relying on pre-defined handcrafted features. How does a Vision Transformer (ViT) work?The performance of a vision transformer model depends on decisions such as that of the optimizer, network depth, and dataset-specific hyperparameters. Compared to ViT, CNNs are easier to optimize.The disparity on a pure transformer is to marry a transformer to a CNN front end. The usual ViT stem leverages a 16*16 convolution with a 16 stride. In comparison, a 3*3 convolution with stride 2 increases the stability and elevates precision.CNN turns basic pixels into a feature map. Later, the feature map is translated by a tokenizer into a sequence of tokens that are then inputted into the transformer. The transformer then applies the attention technique to create a sequence of output tokens.Eventually, a projector reconnects the output tokens to the feature map. The latter allows the examination to navigate potentially crucial pixel-level details. This thereby lowers the number of tokens that need to be studied, lowering costs significantly.Particularly, if the ViT model is trained on huge datasets that are over 14M images, it can outperform the CNNs. If not, the best option is to stick to ResNet or EfficientNet. The vision transformer model is trained on a huge dataset even before the process of fine-tuning. The only change is to disregard the MLP layer and add a new D times KD*K layer, where K is the number of classes of the small dataset.To fine-tune in better resolutions, the 2D representation of the pre-trained position embeddings is done. This is because the trainable liner layers model the positional embeddings. Challenges of Vision TransformersThe challenges of vision transformers are many, and they include issues related to architecture design, generalization, robustness, interpretability, and efficiency.In general, transformer lack some inductive biases compared to CNNs, and rely heavily on massive datasets for large-scale training, which is why the quality of data significantly influences the generalization and robustness of transformer in computer vision tasks.Whilst ViT shows exceptional performance on downstream image classification tasks, for example, VTAB and CIFAR, directly applying the ViT backbone on object detection has failed to surpass the results of CNNsAdditionally, it still remains a challenge to fully understand why transformer work well on visual tasks. Furthermore, developing efficient transformer models for computer vision that can be deployed on resource-limited devices is a challenging issue. Real-World Vision Transformer (ViT) Use Cases and ApplicationsVision transformers have extensive applications in popular image recognition tasks such as object detection, segmentation, image classification, and action recognition. Moreover, ViTs are applied in generative modeling and multi-model tasks, including visual grounding, visual-question answering, and visual reasoning.Video forecasting and activity recognition are all parts of video processing that require ViT. Moreover, image enhancement, colorization, and image super-resolution also use ViT models. Last but not least, ViTs have numerous applications in 3D analysis, such as segmentation and point cloud classification.An example of image segmentation in Sports ConclusionThe vision transformer model uses multi-head self-attention in Computer Vision without requiring image-specific biases. The model splits the images into a series of positional embedding patches, which are processed by the transformer encoder.It does so to understand the local and global features that the image possesses. Last but not least, the ViT has a higher precision rate on a large dataset with reduced training time. What’s nextRead more about related topics and other state-of-the-art methods in machine learning, image processing, and recognition.Optical Character Recognition (OCR)Supervised vs Unsupervised Learning for Computer VisionObject Detection Today: The Definitive GuideYOLOR – You Only Learn One RepresentationIntroduction to MLOps – Methods To Deliver Machine LearningFollow us    Twitter      Linkedin-in      Related Articles      Show more  Deep Face Recognition: An Easy-To-Understand OverviewFace recognition technologies greatly advanced with deep learning-based methods. Read an overview about deep face recognition technology, its applications, and challenges.  Read More » Computer Vision in Sports – Use Cases in 2023A list of the most powerful applications of AI in sports and fitness, from athlete tracking to AI-based movement analysis.  Read More » All-in-one platform to build computer vision applications without code      Show me more       viso.aiProduct Overview Evaluation Guide Feature Index Academy Security Privacy Solutions PricingFeatures Computer Vision Visual Programming Cloud Workspace Analytics Dashboard Device Management End-to-End SuiteIndustries Agriculture Healthcare Manufacturing Retail Security Smart City Technology TransportationResources Blog Learn Evaluation Support WhitepaperAbout Company Careers Terms Contact © 2023 viso.ai Imprint Privacy Terms Follow us   Linkedin      Twitter    viso.ai     Features      Solutions      Company      Blog      Pricing    Request Demo      Play Video Join 6,300+ FellowAI EnthusiastsGet expert AI news 2x a month. Subscribe to the most read Computer Vision Blog.             Subscribe me  You can unsubscribe anytime. See our privacy policy. Build any Computer Vision Application, 10x fasterAll-in-one Computer Vision Platform for businesses to build, deploy and scale real-world applications.   Request Demo   Schedule a live demo           Select...AfghanistanÅland IslandsAlbaniaAlgeriaAmerican SamoaAndorraAngolaAnguillaAntarcticaAntigua and BarbudaArgentinaArmeniaArubaAustraliaAustriaAzerbaijanBahrainBahamasBangladeshBarbadosBelarusBelgiumBelizeBeninBermudaBhutanBolivia, Plurinational State ofBonaire, Sint Eustatius and SabaBosnia and HerzegovinaBotswanaBouvet IslandBrazilBritish Indian Ocean TerritoryBrunei DarussalamBulgariaBurkina FasoBurundiCambodiaCameroonCanadaCape VerdeCayman IslandsCentral African RepublicChadChileChinaChristmas IslandCocos (Keeling) IslandsColombiaComorosCongoCook IslandsCosta RicaCôte d'IvoireCroatiaCubaCuraçaoCyprusCzech RepublicDenmarkDjiboutiDominicaDominican RepublicEcuadorEgyptEl SalvadorEquatorial GuineaEritreaEstoniaEthiopiaFalkland Islands (Malvinas)Faroe IslandsFijiFinlandFranceFrench GuianaFrench PolynesiaFrench Southern TerritoriesGabonGambiaGeorgiaGermanyGhanaGibraltarGreeceGreenlandGrenadaGuadeloupeGuamGuatemalaGuernseyGuineaGuinea-BissauGuyanaHaitiHoly See (Vatican City State)HondurasHong KongHungaryIcelandIndiaIndonesiaIran, Islamic Republic ofIraqIrelandIsle of ManIsraelItalyJamaicaJapanJerseyJordanKazakhstanKenyaKiribatiKoreaKuwaitKyrgyzstanLaoLatviaLebanonLesothoLiberiaLibyaLiechtensteinLithuaniaLuxembourgMacaoMacedoniaMadagascarMalawiMalaysiaMaldivesMaliMaltaMarshall IslandsMartiniqueMauritaniaMauritiusMayotteMexicoMicronesiaMoldovaMonacoMongoliaMontenegroMontserratMoroccoMozambiqueMyanmarNamibiaNauruNepalNetherlandsNew CaledoniaNew ZealandNicaraguaNigerNigeriaNiueNorfolk IslandNorthern Mariana IslandsNorwayOmanPakistanPalauPalestine, State ofPanamaPapua New GuineaParaguayPeruPhilippinesPitcairnPolandPortugalPuerto RicoQatarRéunionRomaniaRussian FederationRwandaSaint BarthélemySaint HelenaSaint Kitts and NevisSaint LuciaSaint Martin (French part)Saint Pierre and MiquelonSaint Vincent and the GrenadinesSamoaSan MarinoSao Tome and PrincipeSaudi ArabiaSenegalSerbiaSeychellesSierra LeoneSingaporeSint Maarten (Dutch part)SlovakiaSloveniaSolomon IslandsSomaliaSouth AfricaSouth SudanSpainSri LankaSudanSurinameSvalbard and Jan MayenSwazilandSwedenSwitzerlandSyrian Arab RepublicTaiwan, Province of ChinaTajikistanTanzania, United Republic ofThailandTimor-LesteTogoTokelauTongaTrinidad and TobagoTunisiaTurkeyTurkmenistanTurks and Caicos IslandsTuvaluUgandaUkraineUnited Arab EmiratesUnited KingdomUnited StatesUruguayUzbekistanVanuatuVenezuela, Bolivarian Republic ofViet NamVirgin Islands, BritishVirgin Islands, U.S.Wallis and FutunaWestern SaharaYemenZambiaZimbabwe   I am looking for  Video analytics Edge computer vision Real-time deep learning  I am interested in  Platform overview Full solution services Evaluation access      Request Demo  Not interested?We’re always looking to improve, so please let us know why you are not interested in using Computer Vision with Viso Suite.     I need more information I think its too difficult I need other features I don't have a use case OtherFind some helpful information or get in touch: > Show me all features > Show me use cases > About the company viso.ai > I have a question     Submit    How it works     Build Apps      Deploy Apps      Overview      Monitor Apps      Manage Apps  Company     About us      Jobs      Contact us      AI Vision      Solutions      Pricing      Blog      Support      Help Center       Get started  We value your privacyWe use cookies to enhance your browsing experience, serve personalized ads or content, and analyze our traffic. By clicking "Accept All", you consent to our use of cookies. Do not sell my personal informationCookie SettingsAcceptManage consent   Close Privacy OverviewThis website uses cookies to improve your experience while you navigate through the website. Out of these, the cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may affect your browsing experience.   Necessary    Necessary   Always Enabled Necessary cookies are absolutely essential for the website to function properly. These cookies ensure basic functionalities and security features of the website, anonymously.CookieDurationDescriptioncookielawinfo-checkbox-advertisement1 yearSet by the GDPR Cookie Consent plugin, this cookie is used to record the user consent for the cookies in the "Advertisement" category .cookielawinfo-checkbox-analytics11 monthsThis cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category "Analytics".cookielawinfo-checkbox-functional11 monthsThe cookie is set by GDPR cookie consent to record the user consent for the cookies in the category "Functional".cookielawinfo-checkbox-necessary11 monthsThis cookie is set by GDPR Cookie Consent plugin. The cookies is used to store the user consent for the cookies in the category "Necessary".cookielawinfo-checkbox-others11 monthsThis cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category "Other.cookielawinfo-checkbox-performance11 monthsThis cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category "Performance".elementorneverThis cookie is used by the website's WordPress theme. It allows the website owner to implement or change the website's content in real-time.JSESSIONIDsessionThe JSESSIONID cookie is used by New Relic to store a session identifier so that New Relic can monitor session counts for an application.viewed_cookie_policy11 monthsThe cookie is set by the GDPR Cookie Consent plugin and is used to store whether or not user has consented to the use of cookies. It does not store any personal data.ZCAMPAIGN_CSRF_TOKENsessionThis cookie is used to distinguish between humans and bots.zfccnsessionZoho sets this cookie for website security when a request is sent to campaigns.  Functional   functionalFunctional cookies help to perform certain functionalities like sharing the content of the website on social media platforms, collect feedbacks, and other third-party features.CookieDurationDescription_zcsr_tmpsessionZoho sets this cookie for the login function on the website.  Performance   performancePerformance cookies are used to understand and analyze the key performance indexes of the website which helps in delivering a better user experience for the visitors.CookieDurationDescription_gat1 minuteThis cookie is installed by Google Universal Analytics to restrain request rate and thus limit the collection of data on high traffic sites.  Analytics   analyticsAnalytical cookies are used to understand how visitors interact with the website. These cookies help provide information on metrics the number of visitors, bounce rate, traffic source, etc.CookieDurationDescription_ga2 yearsThe _ga cookie, installed by Google Analytics, calculates visitor, session and campaign data and also keeps track of site usage for the site's analytics report. The cookie stores information anonymously and assigns a randomly generated number to recognize unique visitors._gat_gtag_UA_177371481_21 minuteSet by Google to distinguish users._gid1 dayInstalled by Google Analytics, _gid cookie stores information on how visitors use a website, while also creating an analytics report of the website's performance. Some of the data that are collected include the number of visitors, their source, and the pages they visit anonymously.CONSENT2 yearsYouTube sets this cookie via embedded youtube-videos and registers anonymous statistical data.zabUserId1 yearThis cookie is set by Zoho and identifies whether users are returning or visiting the website for the first timezabVisitIdone yearUsed for identifying returning visits of users to the webpage.zft-sdc24hoursIt records data about the user's navigation and behavior on the website. This is used to compile statistical reports and heat maps to improve the website experience.zps-tgr-dts1 yearThese cookies are used to measure and analyze the traffic of this website and expire in 1 year.  Advertisement   advertisementAdvertisement cookies are used to provide visitors with relevant ads and marketing campaigns. These cookies track visitors across websites and collect information to provide customized ads.CookieDurationDescriptionVISITOR_INFO1_LIVE5 months 27 daysA cookie set by YouTube to measure bandwidth that determines whether the user gets the new or old player interface.YSCsessionYSC cookie is set by Youtube and is used to track the views of embedded videos on Youtube pages.yt-remote-connected-devicesneverYouTube sets this cookie to store the video preferences of the user using embedded YouTube video.yt-remote-device-idneverYouTube sets this cookie to store the video preferences of the user using embedded YouTube video.  Others   othersOther uncategorized cookies are those that are being analyzed and have not been classified into a category as yet.CookieDurationDescription2d719b1dd3sessionThis cookie has not yet been given a description. Our team is working to provide more information.4662279173sessionThis cookie is used by Zoho Page Sense to improve the user experience.ad2d102645sessionThis cookie has not yet been given a description. Our team is working to provide more information.zc_consent1 yearNo description available.zc_show1 yearNo description available.zsc2feeae1d12f14395b6d5128904ae37461 minuteThis cookie has not yet been given a description. Our team is working to provide more information. Save & Accept                  </pre>
</body>
</html>