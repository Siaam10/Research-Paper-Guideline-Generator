<!DOCTYPE html>
<html>
<head>
<meta charset='utf-8'>
<title>https://towardsdatascience.com/what-are-vision-transformers-and-how-are-they-important-for-general-purpose-learning-edd008545e9e</title>
</head>
<body>
<pre>What Are Vision Transformers And How Are They Important For General Purpose Learning? | by J. Rafid Siddiqui, PhD | Towards Data ScienceOpen in appSign upSign InWriteSign upSign InPublished inTowards Data ScienceJ. Rafid Siddiqui, PhDFollowMay 23, 2022·7 min read·Member-onlySaveWhat Are Vision Transformers And How Are They Important For General Purpose Learning?Exploring The Concept and Experimenting with The Example ApplicationsFigure 1: Vision Transformers in Action (Image by Author)There has been a significant advancement in the field of AI in the past several years. Generative models have been the most successful in the vision domain however, they are built for highly specialized tasks. These specialized learning models require reconstruction or retraining whenever the task is changed. Therefore, the interest in General purpose learning models is increasing. One of such type of models is called Transformers. In this article, we briefly discuss:What is a Transformer?What is a Vision Transformer (ViT)?What are the various applications of ViTs?How can ViTs be used for general purpose learning?1. The Text TransformersThe concept of transformers originated in Natural Language Processing (NLP) applications, whereby the task is to understand the text and draw meaningful conclusions. Transformer models have achieved high performance and have become a de-facto standard in the NLP domain due to their simplicity and generalizability.Figure 2: An example of a transformer for language translation (Image by author)In text processing applications, the data consists of a large sequence of words which come from a fixed set of vocabulary. In a typical transformer architecture, a set of following sequence of steps are performed:► Text is split into a set of words called tokens.► Each token is converted into an encoded/embedded vector (e.g. word2vec)► The position of the word in the sequence is encoded using position embedding [1] and integrated with the word embedding.► The embeddings are fed into the Transformer encoder.► The encoder has a Multi-Layer Self-Attention Network (MSP) which assigns weights to tokens based on their relative importance in the sentence, hence embedding the context.► A Multi-layer Perceptron (MLP) network follows the MSP and encodes the output from the attention network.► There are multiple MSP and MLP blocks along with Norm layers inside the encoder.► A final MLP-Head Layer is added outside the encoder network which provides the logits. Logits can be converted to probabilities by applying an activation layer (e.g. softmax).As we can see that the architecture of a transformer network is generic because the encoder output is not constructed for a specific task (e.g. classification) but rather it provides a general encoding that can be used for a multitude of applications by adding a corresponding MLP Head. This is why transformers are useful in transfer learning and are promising for achieving a general-purpose learning goal.2. Vision Transformers (ViT)The concept of Vision Transformer (ViT) is an extension of the original concept of Transformer, the latter of which is described earlier in this article as text transformer. It is only the application of Transformer in the image domain with slight modification in the implementation in order to handle the different data modality. More specifically, a ViT uses different methods for tokenization and embedding. However, the generic architecture remains the same. An input image is split into a set of image patches, called visual tokens. The visual tokens are embedded into a set of encoded vectors of fixed dimension. The position of a patch in the image is embedded along with the encoded vector and fed into the transformer encoder network which is essentially the same as the one responsible for processing the text input. An example architecture of a ViT in action can be seen in the Figure 3 [2].Figure 3: A demo of a Vision Transformer for Image Classification (Source:Google Research)There are multiple blocks in the ViT encoder and each block consists of three major processing elements: Layer Norm, Multi-head Attention Network (MSP) and Multi-Layer Perceptrons (MLP). Layer Norm keeps the training process on track and let model adapt to the variations among the training images. MSP is a network responsible for generation of attention maps from the given embedded visual tokens. These attention maps help network focus on most important regions in the image such as object(s). The concept of attention maps is same as the one found in the traditional computer vision literature (e.g. saliency maps and alpha-matting).The MLP is a two-layer classification network with GELU (Gaussian Error Linear Unit) at the end. The final MLP block, also termed as MLP head is used as an output of the transformer. An application of softmax on this output can provide classification labels (i.e. if the application is Image Classification).3. ApplicationsDue to their generic nature, the applications of ViTs encompass nearly all aspects of vision. This includes, image classification, image-to-text/text-to-image generation, visual reasoning, association learning and multi-modal learning. In this section, we experiment with the most common and successful applications of ViTs with practical examples.3.1. Image Classification (Image -> Label)The task of image classification is the most common problem in vision. The state-of-art for image classification tasks is CNN (Convolutional Neural Networks) based methods. ViTs don’t produce comparable performance at small to medium dataset however, they have outperformed CNNs on very large datasets [3]. This is due of the fact that CNNs encode the local information in the image more effectively than ViTs due to application of locally restricted receptive fields.Let’s try and experiment with a ViT. We first load a pre-trained model which has been trained on imagenet and then apply it on a set of random images collected from the internet. We pick the five most probable labels for the given image. The result can be seen in Figure 4. The results might not be super-perfect; however, they are very good keeping in mind that the network was not trained on these images.Figure 4: Image Classification Output of a ViT (Image by author)3.2. Image Captioning (Image -> Sentence)A more advanced form of image categorization can be achieved by generating a caption describing the content of an image instead of a one-word label. This has become possible with the use of ViTs. ViTs learn a general representation of a given data modality instead of a crude set of labels, therefore, it is possible to generate a descriptive text for a given image. We will use an implementation of ViT [4] trained on COCO dataset. The results of such captioning can be seen from the Figure 5.Figure 5: Image Caption Generation using a ViT (Image by author)3.3. Contrastive Language-Image Pre-Training (Image <-> Text Snippet)Figure 6: A ViT architecture with Contrastive Language-Image Pre-Training (Source:Google Research)A more advanced application of ViTs is the learning of association that exists between an image and a piece of text. This is the application where the true ability of generalizability is required because it requires an abstract representation of both the text as well as the image. This can be achieved by training two separate transform encoders for text snippet and the image. The encoded image as well as the text features can then be compared for their respective similarity by constructing a cosine-similarity matrix. A relevant implementation for such transformer model is proposed in CLIP (Contrastive Language-Image Pre-Training) [5]. In order to understand this further, we take five sample images and write a small text snippet for each image. Then the text snippets as well as the images are encoded into a set of text and image feature vectors respectively by using the pre-trained ViT model. We calculate the cosine similarity between the text and the image features. This results in an output as seen in Figure 7. As it can be seen that the similarity among the correct image-text pairs is the highest. This type of learning is relatively more general and is a form of transfer learning.Figure 7: Results of a ViT for Image-Text association learning (image by author)4. Final RemarksIn this article, we have explained the concept of text transformers & image transformers. Then we have explored some of the main applications of the transformer models by constructing & implementing the practical examples. If you would like to delve deeper and get your hands-on the code then you can access the respective python notebook and helping code from the git repository. https://github.com/azad-academy/vision-transformersIn the coming articles, we shall look more deeply into the transfer learning and some of the recent advancements in the transformer networks.5. References[1] Mehreen Saeed, A Gentle Introduction To Positional Encoding In Transformer Models , https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/, 2022[2] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”, ICLR, 2021[3] Xiangning Chen, Cho-Jui Hsieh, Boqing Gong ,“When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations”, 2022[4] Saahil et al, CaTr: Image Captioning with Transformers, https://github.com/saahiluppal/catr, 2020[5] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, “Learning Transferable Visual Models From Natural Language Supervision”, 2021Deep LearningMachine LearningTransformersVision TransformerComputer Vision----More from Towards Data ScienceFollowYour home for data science. A Medium publication sharing concepts, ideas and codes.Read more from Towards Data ScienceAboutHelpTermsPrivacyGet the Medium appJ. Rafid Siddiqui, PhD685 FollowersResearch Scientist (AI/ML/CV), Educator, and Innovator. Writes about Deep learning, Computer Vision, Machine Learning, AI, & Philosophy. bit.ly/MLMethodsBookFollowHelpStatusWritersBlogCareersPrivacyTermsAboutText to speech









































</pre>
</body>
</html>